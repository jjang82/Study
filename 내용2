# 과제 지도 -------------

# DX-LSS Seed 과제 지원---------------------------------------------------------
# 작성자 : 백인엽P,  최종 수정일 : 20. 10. 28
# Text encoding : UTF-8

# 세부 환경 설정 ---------------------------------------------------------------

# set Working directory 
getwd()
setwd("D:/#.Secure Work Folder/DX-LSS-Project/pjt/VCM/DAT")
# R Studio에서 Session > Set working directory > choose directory 로 설정 가능

dir()
rm(list = ls())   # Data set 삭제(Global Environment)


# ■ Define ---------------------------------------------------------------------

# Step01 개선 기회 탐색  -------------------------------------------------------
# 정보화된 시스템 중 어느 시스템을 통해 개선 기회를 수시 점검 및 발굴 할 수 있는가?
# VCM 측정값도 엑셀관리, 환경이슈 점검 이슈는 별도 



# Step02 개선 기회 발굴 및 과제 선정--------------------------------------------
# - Big Y - little y 전개를 통해 과제 선정



# Step03 Project Y 선정  -------------------------------------------------------
## PVC 공장 폐수내 VCM 농도를 KPI로 선정
### 매년 PVC 2팀 공정 폐수 VCM배출량은 '18년 8676, 19년 3230으로 20년 548을 목표로하며, VCM함량 초과로 가동 중단 시
# 공정 전체 S/D이 되어야 하는 법적 Risk가 존재, 사외 배출 법적 기준 1ppm 미만으로 개선이 필요. 
### 평균 농도(ppm)  1.3 -> 0.5
### Max 농도(ppm) 45 -> 1
### 표준편차 3.1 -> 0.3



# ■ Measure ----------------------------------------------------------------------
# Step04 데이터 수집 및 검증 계획 수립   ---------------------------------------
# 고객社 분석법(VDA277)과 동일한 조건으로 TVOC Data 수집 계획을 수립함
# 측정 지표 : TVOC(ppm), 측정 설비 : GC 측정 설비, 수집 기간 : '20. 1~3월(1분기)


# Step05 데이터 Set 구성 -------------------------------------------------------
dir()
library(readxl)
z = read_excel("K56폐수운전최적화 Data.xlsx")
#a = read.csv("K56폐수운전최적화 Data.csv", stringsAsFactors=FALSE)
#str(a)

colnames(z)
str(z)
#View(z)
colnames(z)[2]=c("y")
colnames(z)[1]=c("time")

hist(z$y)
plot(z$y)
boxplot(z$y)
summary(z$y)

dim(z)
z = na.omit(z)
dim(z)

# Step06 데이터 취득 시스템(유용성)검증  ---------------------------------------
# Step07 프로세스 현수준 파악  -------------------------------------------------
#Install.packages(SixSigma)
library(SixSigma)
#ss.study.ca(xST, xLT = NA, LSL = NA, USL = NA, Target = NA)
?ss.study.ca(xST=z$y, USL = 0.5, Target = 1.0)
dev.off()

write.csv(z, file="vcm.csv") # 혹은 Minitab으로 이동해서 공정능력 산출
dir()


## project Y인 폐수 Stripping Tower 후단 배출 폐수의 VCM농도 CTQ로 선정함. 


# Step08 개선 목표 설정  -------------------------------------------------------
## TVOC 평균 276ppm(Z bench 0.32) -> 평균 100ppm 이하(Z bench 3.24)


# ■  Analyze ----------------------------------------------------------------------
# Step09 X인자 검증 계획 수립  -------------------------------------------------

# 아래부터는 완료된 코드가 아닙니다. 

# 데이터 수집 계획
# project Y : VM data - LIMS (1회/1일, 07:00)
# x's : 각 TAG data - PIS 공정 Data
# 데이터 Merge 필요 
colnames(z)
str(z)

library(dplyr)
df <- z %>% mutate(LI9118.PV = as.numeric(LI9118.PV),
                   FIC9110.PV = as.numeric(FIC9110.PV),
                   TI9120.PV = as.numeric(TI9120.PV),
                   TI9119.PV= as.numeric(TI9119.PV),
                   TI9118.PV= as.numeric(TI9118.PV),
                   LIC9106.PV= as.numeric(LIC9106.PV),
                   FIC9111.PV= as.numeric(FIC9111.PV),
                   TI1201.PV= as.numeric(TI1201.PV),
                   AGA225A.PV= as.numeric(AGA225A.PV),
                   KVA250A.PV= as.numeric(KVA250A.PV),
                   CMA103.PV= as.numeric(CMA103.PV),
                   KVA230A.PV= as.numeric(KVA230A.PV),
                   WIA201.PV= as.numeric(WIA201.PV),
                   LIC9101A.OP= as.numeric(LIC9101A.OP),
                   LIC9101B.OP= as.numeric(LIC9101B.OP),
                   LIC9102A.OP= as.numeric(LIC9102A.OP),
                   LIC9102B.OP= as.numeric(LIC9102B.OP),
                   LIC9102C.OP= as.numeric(LIC9102C.OP),
                   LIC9102D.OP= as.numeric(LIC9102D.OP),
                   LIC9102E.OP= as.numeric(LIC9102E.OP),
                   LIC9102F.OP= as.numeric(LIC9102F.OP),
                   LIC9151.OP= as.numeric(LIC9151.OP),
                   LIC9112A.OP= as.numeric(LIC9112A.OP),
                   LIC9113A.OP= as.numeric(LIC9113A.OP),
                   LI9207.PV= as.numeric(LI9207.PV),
                   PI9253.PV= as.numeric(PI9253.PV),
                   PI5941.PV= as.numeric(PI5941.PV),
                   LI5251.PV= as.numeric(LI5251.PV)
                   
)

str(df)

df2 = df
colnames(df2)

df2 = df2[,c(1,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58)] #변수 선택
str(df2)

df = df2

str(df)     
summary(df)
                   

# Step10 데이터 취득 및 전처리 실시  -------------------------------------------

# 전처리 도구 불러오기
library(dplyr);library(tidyr)


# Step11 데이터 탐색  ----------------------------------------------------------
#graph분석
pairs(df[,-1])  #time 열을 제외하기 위해 -1입력 함     *df[,-1] 의미 데이터셋[행,열]

#
boxplot(df$y) #상자 그림 그려주는 함수 


boxplot.stats(df$y) #상자 그림의 요소 보여줌

boxplot.stats(df$y)$stats[1]   #Q1-1.5*IQR
boxplot.stats(df$y)$stats[5]   #Q3+1.5*IQR 



df2 = df %>% filter(df$y>boxplot.stats(df$y)$stats[1], df$y<boxplot.stats(df$y)$stats[5])
df = df2


# 데이터 요약
summary(df)

# Step12 핵심인자 선정  --------------------------------------------------------
## 데이터 Set 구분하기
str(df)
dim(df)
nrow(df)
set.seed(7279)
train=sample(nrow(df),nrow(df)*0.7)
train

test=(1:c(nrow(df)))[-train]
test

length(train)
length(test)

df_train = df[train,]
df_test = df[test,]

head(df_train)
head(df_test)



# Step13 분석 모형 검토  -------------------------------------------------------



# ■ Improve ----------------------------------------------------------------------
# Step14 최적 모형 수립  -------------------------------------------------------
## 분석실시(modeling) regression, rf
lm.fit=lm(y~.,data=df[train,-1])
step(lm.fit)
lm.fit_best = lm(y ~ TI9118.PV + KVA250A.PV + LIC9151.OP + PI5941.PV + 
              LI5251.PV, data=df[train,-1])



library(randomForest) ; library(tree)
rf.fit=randomForest(y~.,data=df[train,-1],importance=T)
rf.fit
# importance(rf.fit)
varImpPlot(rf.fit)


# 랜덤포레스트 주요 인자 기준으로 모형 재수립
rf.fit_best=randomForest(y~TI9118.PV
                         + LIC9151.OP
                         + TI1201.PV
                         + LIC9106.PV
                         + KVA250A.PV
                         + LI9207.PV
                         + PI5941.PV
                         + LI5251.PV
                         + LI9118.PV,data=df[train,-1],importance=T)
rf.fit_best
# importance(rf.fit)
varImpPlot(rf.fit_best)



# Step15 모형 검증 및 최적화  --------------------------------------------------

lm_obs = df[test,]$y #실제 관측값
lm_pred = predict(lm.fit_best,newdata=df[test,-1]) # 예측값 
rf_pred = predict(rf.fit_best,newdata=df[test,-1]) # 예측값 

library(DescTools)
MSE(lm_pred, lm_obs)
MSE(rf_pred, lm_obs)

RMSE(lm_pred, lm_obs) # 회귀식이 상대적으로 우수함
RMSE(rf_pred, lm_obs)

(cor(lm_pred,lm_obs))^2  #상관계수 제곱
(cor(rf_pred,lm_obs))^2  #상관계수 제곱


print("Feature range")
for( i in 1:30){
  A = df %>% filter(df[,i]>0) %>% .[,i] 
  B = A[A>boxplot(A)$stats[1]&A<boxplot(A)$stats[5]] %>% range()
  print(data.frame(names=colnames(df)[i] ,lower=B[1],upper=B[2]))
}

table(df$KVA250A.PV)

summary(lm.fit_best) # 부호 다시 체크,  project Y 망소 특성

new=data.frame(TI9118.PV=72644 ,  # 음
               KVA250A.PV=  6, # 음 inf check
               LIC9151.OP= 21.4,  # 음
               PI5941.PV=4.71149 , #양 
               LI5251.PV=6.8  #음
               )


new





#최적조건 구하기- 인자별 Range확인 (예측) 
predict(lm.fit_best,newdata=new)


#최적조건 추가 확인_randomForest

tr.fit = tree(y~TI9118.PV
              + LIC9151.OP
              + TI1201.PV
              + LIC9106.PV
              + KVA250A.PV
              + LI9207.PV
              + PI5941.PV
              + LI5251.PV
              + LI9118.PV,data=df[train,-1])
plot(tr.fit) ; text(tr.fit)

new=data.frame(TI9118.PV=72644 ,  # 음
               LIC9151.OP= 21.4,  # 음
               TI1201.PV = 5.8,
               LIC9106.PV = 31.7 , # 방향 체크 
               KVA250A.PV=  6, # 음
               LI9207.PV = 0.7, # 부호 체크 
               PI5941.PV=4.71149 , #양 
               LI5251.PV=6.8,  #음
               LI9118.PV=30.5  #음
)

predict(rf.fit.best,newdata=new)


# Step16 개선 결과 검증(Pilot Test) --------------------------------------------

# 상기 조건으로 pilot Test 진행 후 결과 체크 





# ■ Control ----------------------------------------------------------------------
# Step17 최적모형 모니터링  ----------------------------------------------------

# Step18 표준화 및 수평전개  ---------------------------------------------------





# 이하 부록 ----------------------------------------------------------------- 





# ● Regression ----------------------------------------------------------
#Regression
library(car)
reg.fit2=lm(Response~Ing2+Ing3+Ing5,data=Ex3.4)
vif(reg.fit2)
summary(reg.fit2)
opar=par(mfrow=c(2,2))
plot(reg.fit2) 

summary( lm(Response~Ing1+Ing2+Ing3+Ing4+Ing5,data=Ex3.4))
vif (lm(Response~Ing1+Ing2+Ing3+Ing4+Ing5,data=Ex3.4))



# ● 연속형 관리도  ----------------------------------------------------------
library(qcc)
Ex5.1=read.table("5.1_Contorl_Xbar_R.txt",header=T)
Ex5.1.1=with( Ex5.1, qcc.groups(Noise, Date))
qcc (Ex5.1.1, type="xbar" )
qcc(Ex5.1.1, type="R")
dev.off()


Ex5.2=read.table("5.2_Contorl_Xbar_R.txt",header=T)
Ex5.2.2=with( Ex5.2, qcc.groups(Noise, Date))
qcc(Ex5.2.2[1:14, ], type="xbar"); qcc(Ex5.2.2[15:28,],type="xbar")
qcc(Ex5.2.2[1:14, ], type="R"); qcc(Ex5.2.2[15:28,],type="R")
dev.off()


Ex5.2_IMR=read.table("5.2_IMR.txt" ,header=T)
qcc(Ex5.2_IMR,type="xbar.one")
Ex5.2_MR=cbind(Ex5.2_IMR[1:29,1],Ex5.2_IMR[2:30,1])
qcc(Ex5.2_MR,type="R")
dev.off()

# ● 이산형 관리도 ----------------------------------------------------------

Ex5.3=read.table("5.3_NP.txt",header=T)
with(Ex5.3,qcc(NC,sizes=50,type="np"))
dev.off()

Ex5.4=read.table("5.4_P.txt",header=T) #교제 오류 있음
with(Ex5.4,qcc(NC,sizes=Sample,type="p"))
dev.off()

Ex5.5=read.table("5.5_C.txt",header=T)
with(Ex5.5,qcc(NC ,type="c"))

Ex5.6=read.table("5.6_U.txt" ,header=T)
with(Ex5.6,qcc(NC,sizes=No.Unit,type="u"))


# 과제 지도 2

# DX-LSS Seed 과제 지원---------------------------------------------------------
# 작성자 : 백인엽P,  최종 수정일 : 20. 10. 28
# Text encoding : UTF-8

# 세부 환경 설정 ---------------------------------------------------------------

# set Working directory 
getwd()
setwd("D:/#.Secure Work Folder/DX-LSS-Project/pjt/TVOC/DAT")
# R Studio에서 Session > Set working directory > choose directory 로 설정 가능

dir()
rm(list = ls())   # Data set 삭제(Global Environment)


# ■ Define ---------------------------------------------------------------------

# Step01 개선 기회 탐색  -------------------------------------------------------
# - LIMS 시스템에서 과제 도출 가능


# Step02 개선 기회 발굴 및 과제 선정--------------------------------------------
# - Big Y - little y 전개를 통해 과제 선정


# Step03 Project Y 선정  -------------------------------------------------------
## KPI  TVOC 300ppm --> 200ppm,   KPI와 CTQ 동일
## Total Volatile Organic Compound
## 자동차 내외장재, 차에서 나는 냄새를 관리, TVOC라는 냄새를 유발하는 휘발성 유기 화합물을 CTQ로 하는 개선 과제임 공정 특성상 노말헥산을 사용하고 있어 솔벤트에 있어 잔류하는 휘발성 액체가 남아 있어 공중합체로 투입되는 옥텐으로 있어  잔류된 노말헥산, 옥텐을 제거 하는 과제임
## TVOC는 제품 Lot당 1개씩 측정됨. 공정 데이터와 비교 데이터가 작아 대용지표 발굴 필요 


# ■ Measure ----------------------------------------------------------------------
# Step04 데이터 수집 및 검증 계획 수립   ---------------------------------------
# 고객社 분석법(VDA277)과 동일한 조건으로 TVOC Data 수집 계획을 수립함
# 측정 지표 : TVOC(ppm), 측정 설비 : GC 측정 설비, 수집 기간 : '20. 1~3월(1분기)


# Step05 데이터 Set 구성 -------------------------------------------------------
z = read.csv("TVOC_M.CSV")
colnames(z)
colnames(z)[3]=c("y")


# Step06 데이터 취득 시스템(유용성)검증  ---------------------------------------
# Step07 프로세스 현수준 파악  -------------------------------------------------
#Install.packages(SixSigma)
library(SixSigma)
#ss.study.ca(xST, xLT = NA, LSL = NA, USL = NA, Target = NA)
ss.study.ca(xST=z$y, USL = 300, Target = 200)

rm(list=ls())
dev.off()


## project Y인 TVOC 분석 주기에 따른 공정 응답성 지연을 고려하여 VM을 대용지표로 선정함
## cor(z$y, lims$vm) ; pairs(z$y, lims$vm)
## TVOC vs VM에 대한 pearson 상관계수 0.618(p-value 0.000)임 

## VM 선정 배경
### 1. 분석 시간이 짧아 피드백이 빠르며, 공정 제어 지표로서 적절함
### 2. 생산 제품의 현재 휘발분 함량 상태를 즉각적으로 대변함
### 3. 동일주기 측정으로 (매일 07:00) 공정 상태와의 Data 연계가 보다 용이하여 예측 모델의 신뢰성 확보 가능


# 대용 지표 인 VM의 현수준 파악
# 측정 지표 : VM(ppm), 측정 설비 : 약식 분석, 1일 1회(07:00), 수집 기간 : '20. 1~3월(1분기), 측정 위치 : Hold-up bin, 분석 원리 : 가열 전후 Weight감소량(질량 분율), 기준 온도 120도, 총분석 시간 : 2hr


lims1 = read.csv("POE_TVOC_LIMS_LINE1_20190104-20200404.csv", stringsAsFactors = FALSE)
str(lims1)
colnames(lims1)
lims1 = lims1[,c(1,6)]


lims2 = read.csv("POE_TVOC_LIMS_LINE2_20190104-20200404.csv", stringsAsFactors = FALSE) 
str(lims2)
colnames(lims2)
lims2 = lims2[,c(1,6)]


lims = rbind(lims1, lims2)
str(lims)
lims$TIME = as.POSIXct(lims$TIME)
lims$VM = ifelse(lims$VM >= 0,lims$VM,"") #문자열 제거, 음수 제거
lims$VM = as.numeric(lims$VM)
lims = na.omit(lims)
str(lims)
# View(lims)

hist(lims$VM)
plot(lims$TIME, lims$VM)


library(SixSigma)
#ss.study.ca(xST, xLT = NA, LSL = NA, USL = NA, Target = NA)
ss.study.ca(xST=lims$VM, USL = 4000, Target = 3000)

# Step08 개선 목표 설정  -------------------------------------------------------
## TVOC 평균 276ppm(Z bench 0.32) -> 평균 100ppm 이하(Z bench 3.24)


# ■  Analyze ----------------------------------------------------------------------
# Step09 X인자 검증 계획 수립  -------------------------------------------------

# 데이터 수집 계획
# project Y : VM data - LIMS (1회/1일, 07:00)
# x's : 각 TAG data - PIS 공정 Data
# 데이터 Merge 필요 

dir()
TAG = read.csv("TAG_INFO.csv")
head(TAG)

pis1 = read.csv("POE_TVOC_PIS_LINE1_20190104-20200404.csv", stringsAsFactors = FALSE)
str(pis1)

#Line2는 나중에 Line1부터 분석
#pis2 = read.csv("POE_TVOC_PIS_LINE2_20190104-20200404.csv", stringsAsFactors = FALSE)
#str(pis2)

library(dplyr)
pis1$TIME = as.POSIXct(pis1$TIME)
str(pis1)

pis1$TIME = pis1$TIME %>% format(.,"%Y-%m-%d %H") 

colnames(pis1)


#pis_test = pis1[1:10000,] 일부 데이터만 테스트 
pis_test = pis1

head(pis_test)
colnames(pis_test)

TAG

d1 = pis_test %>% group_by(TIME) %>% summarise(EL1PIC12102_2 = mean(EL1PIC12102_2),
                                                EL1FIC12302CA = mean(EL1FIC12302CA),
                                                EL1FIC12302FC = mean(EL1FIC12302FC),
                                                EL1FIC12302GE = mean(EL1FIC12302GE),
                                                EL1FIC12302HA = mean(EL1FIC12302HA),
                                                EL1FIC12302KC = mean(EL1FIC12302KC),
                                                EL1FIC12302AA = mean(EL1FIC12302AA),
                                                EL1FIC12302BC = mean(EL1FIC12302BC),
                                                EL1FIC12121 = mean(EL1FIC12121),
                                                EL1FIC12103 = mean(EL1FIC12103),
                                                EL1FIC12105 = mean(EL1FIC12105),
                                                EL1FIC12201 = mean(EL1FIC12201),
                                                EL1FIC12107 = mean(EL1FIC12107),
                                                EL1TIC12203 = mean(EL1TIC12203),
                                                EL1TIC12220 = mean(EL1TIC12220),
                                                EL1PIC12201 = mean(EL1PIC12201),
                                                EL1PIC12401_1 = mean(EL1PIC12401_1),
                                                EL1FI12114 = mean(EL1FI12114),
                                                EL1TI12401 = mean(EL1TI12401),
                                                EL1TIC12608 = mean(EL1TIC12608),
                                                EL1PIC12609 = mean(EL1PIC12609),
                                                EL1PIC13405 = mean(EL1PIC13405),
                                                EL1TI13102 = mean(EL1TI13102),
                                                EL1PIC13201 = mean(EL1PIC13201),
                                                EL1PIC13202 = mean(EL1PIC13202),
                                                EL1PIC13203 = mean(EL1PIC13203),
                                                EL1PI13455 = mean(EL1PI13455),
                                                EL1PI13456 = mean(EL1PI13456),
                                                EL1PI13457 = mean(EL1PI13457),
                                                EL1PI13451 = mean(EL1PI13451),
                                                EL1PI13452 = mean(EL1PI13452),
                                                EL1PI13453 = mean(EL1PI13453),
                                                EL1TI13301 = mean(EL1TI13301),
                                                EL1TI13302 = mean(EL1TI13302),
                                                EL1TI13303 = mean(EL1TI13303),
                                                EL1TI13304 = mean(EL1TI13304),
                                                EL1TI13305 = mean(EL1TI13305),
                                                EL1TI13306 = mean(EL1TI13306),
                                                EL1TI13352 = mean(EL1TI13352),
                                                EL1TI13350 = mean(EL1TI13350),
                                                EL1TI13313 = mean(EL1TI13313),
                                                EL1TI13353 = mean(EL1TI13353),
                                                EL1TI13354 = mean(EL1TI13354),
                                                EL1TI13355 = mean(EL1TI13355),
                                                EL1TI13356 = mean(EL1TI13356),
                                                EL1PI13321 = mean(EL1PI13321),
                                                EL1PI13305 = mean(EL1PI13305),
                                                EL1PI13307 = mean(EL1PI13307),
                                                EL1AI13301 = mean(EL1AI13301),
                                                EL1FI13208 = mean(EL1FI13208), 
                                                EL1TIC13329 = mean(EL1TIC13329),
                                                EL1PI13701 = mean(EL1PI13701),
                                                EL1FI13701 = mean(EL1FI13701),
                                                EL1TIC13701 = mean(EL1TIC13701))



# pis_test$TIME = pis_test$TIME %>% format(.,"%Y-%m-%d %H:%M") 
d1[,1] = paste0(d1$TIME,":00")
d1$TIME = as.POSIXct(d1$TIME)
head(d1)
str(d1)


TAG
# Time Lag 반영
E1 = d1[1:13] #time Lag 4hr
E2 = d1[,c(1,15:19)] #time Lag 3hr
E3 = d1[,c(1,20)] #time Lag 2.5hr
E4 = d1[,c(1,21:23)] #time Lag 1.5hr
E5 = d1[,c(1,24)] #time Lag 1.0hr
E6 = d1[,c(1,25:51)] #time Lag 0.5hr
E7 = d1[,c(1,51:54)]  #time Lag 0.0hr




# 4hr 14400, 3hr 10800, 2.5hr 9000, 1.5hr 5400, 1hr 3600, 0.5hr 1800
E1$TIME = E1$TIME - 14400
E2$TIME = E2$TIME - 10800
E3$TIME = E3$TIME - 9000
E4$TIME = E4$TIME - 5400
E5$TIME = E5$TIME - 3600
E6$TIME = E6$TIME -1800

head(E1$TIME,1)
head(E2$TIME,1)
head(E3$TIME,1)
head(E4$TIME,1)
head(E5$TIME,1)
head(E6$TIME,1)
head(E7$TIME,1)

E1$TIME = E1$TIME %>% format(.,"%Y-%m-%d %H")
E2$TIME = E2$TIME %>% format(.,"%Y-%m-%d %H")
E3$TIME = E3$TIME %>% format(.,"%Y-%m-%d %H")
E4$TIME = E4$TIME %>% format(.,"%Y-%m-%d %H")
E5$TIME = E5$TIME %>% format(.,"%Y-%m-%d %H")
E6$TIME = E6$TIME %>% format(.,"%Y-%m-%d %H")
E7$TIME = E7$TIME %>% format(.,"%Y-%m-%d %H") 


lims$TIME = lims$TIME %>% format(.,"%Y-%m-%d %H") 

str(E1$TIME)
str(lims$TIME)

H1 = merge(lims,E1, by= "TIME")
H2 = merge(H1,E2, by= "TIME")
H3 = merge(H2,E3, by= "TIME")
H4 = merge(H3,E4, by= "TIME")
H5 = merge(H4,E5, by= "TIME")
H6 = merge(H5,E6, by= "TIME")
H7 = merge(H6,E7, by= "TIME")

head(H7)
# View(H7)

write.csv(H7, file = "H7.csv")


H8  = H7[,-c(18,33)]
H9  = H7[,-33]

sum(is.na(H8))
H8 = na.omit(H8)

df= H8

# save(df, file = "df")
rm(list=ls())
load("df")

# Step10 데이터 취득 및 전처리 실시  -------------------------------------------

# 전처리 도구 불러오기
library(dplyr);library(tidyr)


# Step11 데이터 탐색  ----------------------------------------------------------
#graph분석
str(df)
pairs(df[,-1])


library(dplyr)
df2 = df[,-1]
colnames(df2)
df3 = df2[,c(1:26)]
df4 = df2[,c(1,27:52)]

pairs(df3 %>% sample_n(min(1000, nrow(df3))),
      lower.panel = function(x,y){ points(x,y);abline(0,1,col='red')},
      upper.panel = panel.cor) #Panel.cor를 사용하려면 아래 Function을 실행해야함.

pairs(df3 %>% sample_n(min(1000, nrow(df3))),
      lower.panel = function(x,y){ points(x,y);abline(0,1,col='red')},
      upper.panel = panel.cor) #Panel.cor를 사용하려면 아래 Function을 실행해야함.


#help(pairs)의 example에서 Function이 만들어져 있고, 활용함)
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- abs(cor(x, y))
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste0(prefix, txt)
  if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
  text(0.5, 0.5, txt, cex = cex.cor * r)
}


# 데이터 요약
summary(df)

# Step12 핵심인자 선정  --------------------------------------------------------
## 데이터 Set 구분하기
str(df)
dim(df)
nrow(df)
set.seed(7279)
train=sample(nrow(df),nrow(df)*0.7)
train

test=(1:c(nrow(df)))[-train]
test

length(train)
length(test)


#df_train = df[train,] #교육 목적으로 의미 전달하여 보여줌. 
#df_test = df[test,] #교육 목적으로 의미 전달하여 보여줌. 

#head(df_train)
#head(df_test)


# Step13 분석 모형 검토  -------------------------------------------------------


# ■ Improve ----------------------------------------------------------------------
# Step14 최적 모형 수립  -------------------------------------------------------
## 분석실시(modeling) regression, rf
colnames(df)[2] = "y"
colnames(df)

lm.fit=lm(y~.,data=df[train,-1])
step(lm.fit)


lm.fit_best = lm(y ~ EL1PIC12102_2 + EL1FIC12302CA + EL1FIC12302GE + 
                 EL1FIC12302KC + EL1FIC12302BC + EL1FIC12103 + EL1FIC12105 + 
                 EL1FIC12201 + EL1TIC12203 + EL1TIC12220 + EL1PIC12201 + EL1FI12114 + 
                 EL1PIC13405 + EL1PIC13201 + EL1PIC13203 + EL1PI13455 + EL1PI13457 + 
                 EL1TI13302 + EL1TI13305 + EL1TI13306 + EL1TI13350 + EL1TI13353 + 
                 EL1TI13354 + EL1TI13355 + EL1TI13356 + EL1PI13321 + EL1AI13301 + 
                 EL1FI13208.x + EL1FI13208.y + EL1TIC13329, data=df[train,-1])


library(car)
vif(lm.fit_best)
# EL1FIC12103 EL1TIC12203  EL1PI13455    EL1PI13457 EL1TI13302 L1TI13305    EL1TI13306    EL1TI13350    EL1TI13353    EL1TI13354    EL1TI13355    EL1TI13356  EL1FI13208.x  EL1FI13208.y



library(glmnet)
xx <- model.matrix(y~.^2-1, df[,-1])
# 첨자 허용 범위를 벗어났습니다. 오류가 발생함. 
x <- xx[train,]
y <- df[train,]$y
glimpse(x)

?cv.glmnet
data_cvfit <- cv.glmnet(x, y)
plot(data_cvfit) # 람다가 왼쪽에서 오른쪽 증가함에 따라 모수의 개수는 줄어들고 모형 간단

coef(data_cvfit, s = c("lambda.1se")) 
a = coef(data_cvfit, s = c("lambda.min")) #해석보단 예측이 초점이므로 이것을 사용 

# alpha = 1.0(라쏘 모형), alpha = 0.0(능동 회귀), alpha = 0.5 (일래스틱넷 모형)

# predict.cv.glmnet(data_cvfit, s="lambda.min", newx = df[1:5,]) #함수 없다고 나옴. 
predict(data_cvfit, s="lambda.min", newx = x[1:5,]) # 인터넷 서칭 책과 다르게 나옴. 

y_obs <- df[train,]$y
yhat_glmnet <- predict(data_cvfit, s="lambda.min", newx=xx[train,])
yhat_glmnet <- yhat_glmnet[,1] #change to a vector from[n*1] matrix

library(DescTools)
RMSE(y_obs, yhat_glmnet)


ridge.lm=glmnet(x,y,alpha=0, lambda=grid)
summary(ridge.mod)

cv.out=cv.glmnet(x,y,alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
ridge.pred=predict(ridge.mod, s=bestlam,x)
ridge.pred


log(158)


library(randomForest) ; library(tree)
rf.fit=randomForest(y~.,data=df[train,-1],importance=T)
rf.fit

# importance(rf.fit)
varImpPlot(rf.fit)

importance(rf.fit)

rf.fit_best=randomForest(y ~ EL1PI13457 +
                      EL1PI13451 +
                      EL1PI13456 +
                      EL1PIC13203 + 
                      EL1PI13455 + 
                      EL1PI13452 +
                      EL1FIC12302GE +
                      EL1PIC13201, data=df[train,-1],importance=T)


rf.fit_best

# Step15 모형 검증 및 최적화  --------------------------------------------------

lm_obs = df[test,]$y #실제 관측값
lm_pred = predict(lm.fit_best,newdata=df[test,-1]) # 예측값 
rf_pred = predict(rf.fit_best,newdata=df[test,-1]) # 예측값 


library(DescTools)
MSE(lm_pred, lm_obs)
MSE(rf_pred, lm_obs)


RMSE(lm_pred, lm_obs)
RMSE(rf_pred, lm_obs)

(cor(lm_pred,lm_obs))^2  #상관계수 제곱
(cor(rf_pred,lm_obs))^2  #상관계수 제곱

colnames(df)
df_range = df %>%  select(EL1PI13457,
                          EL1PI13451,
                          EL1PI13456,
                          EL1PIC13203,
                          EL1PI13455, 
                          EL1PI13452, 
                          EL1FIC12302GE, 
                          EL1PIC13201)
colnames(df_range)
ncol(df_range)
print("Feature range")
for( i in 1:8){
  A = df_range %>% filter(df_range[,i]>0) %>% .[,i] 
  B = A[A>boxplot(A)$stats[1]&A<boxplot(A)$stats[5]] %>% range()
  print(data.frame(names=colnames(df_range)[i] ,lower=B[1],upper=B[2]))
}

library(rpart)
fit.dt <- rpart(y ~ EL1PI13457 +
                  EL1PI13451 +
                  EL1PI13456 +
                  EL1PIC13203 + 
                  EL1PI13455 + 
                  EL1PI13452 +
                  EL1FIC12302GE +
                  EL1PIC13201, data=df[train,-1])

library(rpart.plot)
rpart.plot(fit.dt)

summary(lm.fit_best) # 부호 다시 체크 

new=data.frame(EL1PI13457=10 , # 양
               EL1PI13451=12 , # 음
               EL1PI13456=13 , # 
               EL1PIC13203= ,
               EL1PI13455= , 
               EL1PI13452= , 
               EL1FIC12302GE= , 
               EL1PIC13201=  )         #양 


 
new

#최적조건 구하기- 인자별 Range확인 (예측) 
predict(lm.fit_best,newdata=new)
# predict(rf.fit,newdata=new)


# Step16 개선 결과 검증(Pilot Test) --------------------------------------------




# ■ Control ----------------------------------------------------------------------
# Step17 최적모형 모니터링  ----------------------------------------------------

# Step18 표준화 및 수평전개  ---------------------------------------------------





# 이하 부록 ----------------------------------------------------------------- 


# ● Regression ----------------------------------------------------------
#Regression
library(car)
reg.fit2=lm(Response~Ing2+Ing3+Ing5,data=Ex3.4)
vif(reg.fit2)
summary(reg.fit2)
opar=par(mfrow=c(2,2))
plot(reg.fit2) 

summary( lm(Response~Ing1+Ing2+Ing3+Ing4+Ing5,data=Ex3.4))
vif (lm(Response~Ing1+Ing2+Ing3+Ing4+Ing5,data=Ex3.4))



# ● 연속형 관리도  ----------------------------------------------------------
library(qcc)
Ex5.1=read.table("5.1_Contorl_Xbar_R.txt",header=T)
Ex5.1.1=with( Ex5.1, qcc.groups(Noise, Date))
qcc (Ex5.1.1, type="xbar" )
qcc(Ex5.1.1, type="R")
dev.off()


Ex5.2=read.table("5.2_Contorl_Xbar_R.txt",header=T)
Ex5.2.2=with( Ex5.2, qcc.groups(Noise, Date))
qcc(Ex5.2.2[1:14, ], type="xbar"); qcc(Ex5.2.2[15:28,],type="xbar")
qcc(Ex5.2.2[1:14, ], type="R"); qcc(Ex5.2.2[15:28,],type="R")
dev.off()


Ex5.2_IMR=read.table("5.2_IMR.txt" ,header=T)
qcc(Ex5.2_IMR,type="xbar.one")
Ex5.2_MR=cbind(Ex5.2_IMR[1:29,1],Ex5.2_IMR[2:30,1])
qcc(Ex5.2_MR,type="R")
dev.off()

# ● 이산형 관리도 ----------------------------------------------------------

Ex5.3=read.table("5.3_NP.txt",header=T)
with(Ex5.3,qcc(NC,sizes=50,type="np"))
dev.off()

Ex5.4=read.table("5.4_P.txt",header=T) #교제 오류 있음
with(Ex5.4,qcc(NC,sizes=Sample,type="p"))
dev.off()

Ex5.5=read.table("5.5_C.txt",header=T)
with(Ex5.5,qcc(NC ,type="c"))

Ex5.6=read.table("5.6_U.txt" ,header=T)
with(Ex5.6,qcc(NC,sizes=No.Unit,type="u"))


# 멘토링 시 질문에 대한 답변들

# - LIMS는 품질 데이터는 시간단위 데이터가 아니므로, 
# LIMS데이터랑 앞뒤로 평균 데이터 만드는 함수
# 기준열을 만들어서 mutate와 summerize 함수 사용
mtcars %>%  group_by(., time) %>% summarise(mpg=mean(mpg))

# 품질 수준을 Glass 범주형으로 나눠서 품질 Good, Bad Class 나누는 것을 
# if 함수를 엑셀에서 하는 것 등
ifelse(lims$VM <= 3000, "good", "bad")

#time <- time %>% mutate(time2 = time -  16200)


#특정 시간까지 끊어 낼때, 
library(dplyr)
std <- "2020-10-14 04:44:49"
str(lims)
tmp <- lims %>% filter(time > strptime(std,"%Y-%m-%d %H:%M:%S"))

# 과제 지도 3

# DX-LSS Seed 과제 지원---------------------------------------------------------
# 작성자 : 백인엽P,  최종 수정일 : 20. 11. 6
# Text encoding : UTF-8

# 세부 환경 설정 ---------------------------------------------------------------

# set Working directory 
getwd()
setwd("D:/#.Secure Work Folder/DX-LSS-Project/pjt/TurboBlower/DAT")
# R Studio에서 Session > Set working directory > choose directory 로 설정 가능

dir()
# rm(list = ls())   # Data set 삭제(Global Environment)


# ■ Define ---------------------------------------------------------------------

# Step01 개선 기회 탐색  -------------------------------------------------------
# - LIMS 시스템에서 과제 도출 가능?


# Step02 개선 기회 발굴 및 과제 선정--------------------------------------------
# - Big Y - little y 전개를 통해 과제 선정?


# Step03 Project Y 선정  -------------------------------------------------------
## KPI
## CTQ


# ■ Measure ----------------------------------------------------------------------
# Step04 데이터 수집 및 검증 계획 수립   ---------------------------------------
# 

# Step05 데이터 Set 구성 -------------------------------------------------------
library(readxl)

# 일자별 데이터 추가해서 분석 필요함. 
dir()

z1 = read_excel("20201025.xls")
z2 = read_excel("20201026.xls")
z3 = read_excel("20201027.xls")
z4 = read_excel("20201028.xls")
z5 = read_excel("20201029.xls")
z6 = read_excel("20201030.xls")
z7 = read_excel("20201031.xls")
z8 = read_excel("20201101.xls")
head(z1)
dim(z1) ; dim(z2) ; dim(z3) ; dim(z4); dim(z5) ; dim(z6) ; dim(z7) ; dim(z8)

z1 = z1[2:nrow(z1),]
z2 = z2[2:nrow(z2),]
z3 = z3[2:nrow(z3),]
z4 = z4[2:nrow(z4),]
z5 = z5[2:nrow(z5),]
z6 = z6[2:nrow(z6),]
z7 = z7[2:nrow(z7),]
z8 = z8[2:nrow(z8),]

z = rbind(z1,z2,z3,z4,z5,z6,z7,z8)
dim(z)
# View(z)


head(z)
str(z)
colnames(z)
colnames(z)[1:10]=c("time", "f1","f2","tmp","do","oper","mlss","toc1","toc2","TN")
colnames(z)[5]=c("y")
colnames(z)

dim(z)
dim(na.omit(z))


# 데이터 변환에 실패함.. 이유는 아직 파악 못해서 변경시키지 못함. 
# z[,1] = as.POSIXct(z$time) #LOGGING - 시간에 따른 변화 체크 
# as.POSIXct(z$time, format='%Y%m%d%H%M%S', origin ='1970-01-01', tz="KST") 
# as.POSIXct(z$time, format='%Y%m%d%H%M%S', origin ='1970-01-01', tz="KST") 

#library(lubridate)
#ymd(z$time)
#z$time

z[,1] = as.POSIXct(z$time)#LOGGING - 시간에 따른 변화 체크 변환이 안됨
z[,2] = as.numeric(z$f1) # 유입수 펌프 - 토출유량 #조절 가능 함
z[,3] = as.numeric(z$f2) # 유입수 토출 -  # 
z[,4] = as.numeric(z$tmp) # 폭기조#1 - 폭기조 온도         #조절 가능 
z[,5] = as.numeric(z$y) # 폭기조#2 - 폭기조 DO         #DO를 일정 수준으로유지하고 싶음.
z[,6] = as.numeric(z$oper) # 폭기조 블로워 - 가동률     #가동을 어떻게 해야하는지 알고 싶은 X임
z[,7] = as.numeric(z$mlss) # 폭기조#3 - 폭기조 MLSS    # DO에 영향 미치고, 조절 가능한 항목
z[,8] = as.numeric(z$toc1) # 원수 - (TOC)     #조절 불가
z[,9] = as.numeric(z$toc2) # 방류수 -  (TOC)   #조절 불가 
z[,10] = as.numeric(z$TN) # 방류수 - (TN)     #조절 불가 




#폭기조 블로워 가동률에 대한 중요 변수로 고려하고 있음. 5,6번 영향성을 중요하게 보고 있음. 


# Step06 데이터 취득 시스템(유용성)검증  ---------------------------------------
dim(z)
z1 = z  # 원자료 보존을위해 변수 지정
z1 = na.omit(z1) # 결측치 제거
dim(z1) # 결측치가 없어서 그대로  데이터 셋명 z를 그대로 이용함. 
# z = z1

boxplot(z$y)
boxplot.stats(z$y)$stats[5]
boxplot.stats(z$y)$stats

library(dplyr)
z1 = z %>% filter(boxplot.stats(z$y)$stats[1]<z$y, boxplot.stats(z$y)$stats[5]>z$y)
# z1 = z %>% filter(2.525<z$y, 5.9375>z$y)

head(z1)
hist(z1$y)

plot(z$time, z$y)
hist(z$y)
boxplot(z$y)


# Step07 프로세스 현수준 파악  -------------------------------------------------

#rm(list=ls())
#dev.off()

## project Y는 DO로 선정함

## DO 선정 배경
### 1. 분석 시간이 짧아 피드백이 빠르며, 공정 제어 지표로서 적절함
### 2. 생산 제품의 현재 휘발분 함량 상태를 즉각적으로 대변함
### 3. 동일주기 측정으로 (매일 07:00) 공정 상태와의 Data 연계가 보다 용이하여 예측 모델의 신뢰성 확보 가능

# DO 수집 방법 HMI 시스템(실시간, 주기:?)


#Install.packages(SixSigma)
library(SixSigma)
#ss.study.ca(xST, xLT = NA, LSL = NA, USL = NA, Target = NA)
ss.study.ca(xST=z$y, USL = 3, Target = 2)



# Step08 개선 목표 설정  -------------------------------------------------------
## DO 평균 5.6ppm(Z bench : -1.9) -> 평균 2ppm 이하(Z bench )



# ■  Analyze ----------------------------------------------------------------------
# Step09 X인자 검증 계획 수립  -------------------------------------------------

# 데이터 수집 계획
# project Y : DO data - HMI시스템 (매 10분당)

# x's : HMI system, 기온 기상청 데이터, 폭기조내 MLSS 농도는 현재 미측정
# 최종 data set 구성 필요. 

# Measure단계에 처리하여 별도 처리 없음 



# Step10 데이터 취득 및 전처리 실시  -------------------------------------------

# 전처리 도구 불러오기
library(dplyr);library(tidyr)


# Step11 데이터 탐색  ----------------------------------------------------------
#graph분석
df = z1
pairs(df[,-1])
# 데이터 요약
summary(df)



# 해당 부분은 옵션으로 별도의 교육은 없이 진행.   
library(ggplot2)
library(dplyr)
colnames(df)

# DO와 변수와의 관계 체크   
df %>% ggplot(aes(oper,y))+geom_point()
df %>% ggplot(aes(f1,y))+geom_point()  # 전체 중심의 변화 영향 있음
df %>% ggplot(aes(f2,y))+geom_point()
df %>% ggplot(aes(tmp,y))+geom_point() # 시계열에 따른 변화에 유사해서 추가 데이터 필요
df %>% ggplot(aes(mlss,y))+geom_point()
df %>% ggplot(aes(TN,y))+geom_point()
df %>% ggplot(aes(toc1,y))+geom_point()
df %>% ggplot(aes(toc2,y))+geom_point() # toc2에 전체 평균의 이동이 관찰됨. 

# 시간에 따른 경향성 변수 경향성 체크  
df %>% ggplot(aes(time,oper))+geom_point() # 시계열에 따라 수치가 점점 내려가는 방향으로 조정, Y와의 영향성은 데이터를 추가 관찰 필요함. 
df %>% ggplot(aes(time,f1))+geom_point()
df %>% ggplot(aes(time,f1))+geom_point(aes(col=y)) #낮아지는 변곡점에 영향은 낮아보임. 

df %>% ggplot(aes(time,f2))+geom_point()
df %>% ggplot(aes(time,f2))+geom_point(aes(col=y)) #낮아지는 변곡점에 영향은 낮아보임. 

df %>% ggplot(aes(time,tmp))+geom_point()
df %>% ggplot(aes(time,tmp))+geom_point(aes(col=y)) #낮아지는 변곡점에 영향은 낮아보임. 

df %>% ggplot(aes(time,mlss))+geom_point()
df %>% ggplot(aes(time,mlss))+geom_point(aes(col=y))

df %>% ggplot(aes(time,TN))+geom_point()
df %>% ggplot(aes(time,toc1))+geom_point()
df %>% ggplot(aes(time,toc2))+geom_point()


# 변곡점에서 f2의 값이 영향을 주는 것으로 보여지지만, 지속되지 않음. 
df %>% ggplot(aes(time,y))+geom_point(aes(col=f2))

# 가장 낮은 것은 폭기조 플로워 작동시 가장 낮게 떨어지지만 경향은 유지됨
df %>% ggplot(aes(time,y))+geom_point(aes(size=oper, col=TN))

df %>% ggplot(aes(time,y))+geom_point(aes(size=oper, col=tmp))

# 전체 경향이 높은 온도에서 DO가 낮아지는 경향을 보이지만, 급격히 낮아진 부분은 설명 안됨
df %>%ggplot(aes(time,y))+geom_point(aes(size=f1, col=tmp))

# 폭기조 f1, f2도 낮아진 영역에서 영향은 관찰되지만 핵심은 아님
df %>% ggplot(aes(time,y))+geom_point(aes(size=f1, col=f2))

# 원수와 방류수가 모두 영향을 미치고 있어서 효과 구분이 안되고 있음
df %>% ggplot(aes(time,y))+geom_point(aes(size=toc1, col=toc2))

# Toc2의 영향과 TN의 영향을 모두 받는 것으로 보여짐 
df %>% ggplot(aes(time,y))+geom_point(aes(size=toc2, col=TN))


# boxplot을 위해서 가동률을 Factor 데이터로 변환하여 확인 
df_new = df
df_new$oper = as.factor(df_new$oper)

df_new %>% ggplot(aes(oper,y)) + geom_jitter(col='gray') + geom_boxplot(alpha=0.5)
#df_new %>% ggplot(aes(oper,y)) + geom_jitter(col='gray') + geom_boxplot(alpha=0.5)+coord_flip()

df_new %>% ggplot(aes(oper,y)) + geom_jitter(aes(col=tmp)) + geom_boxplot(alpha=0.3)
df_new %>% ggplot(aes(oper,y)) + geom_jitter(aes(col=f1)) + geom_boxplot(alpha=0.3)
df_new %>% ggplot(aes(oper,y)) + geom_jitter(aes(col=f2)) + geom_boxplot(alpha=0.3)
df_new %>% ggplot(aes(oper,y)) + geom_jitter(aes(col=mlss)) + geom_boxplot(alpha=0.3)

df_new %>% ggplot(aes(oper,y)) + geom_jitter(aes(col=TN)) + geom_boxplot(alpha=0.3)
df_new %>% ggplot(aes(oper,y)) + geom_jitter(aes(col=toc1)) + geom_boxplot(alpha=0.3)
df_new %>% ggplot(aes(oper,y)) + geom_jitter(aes(col=toc2)) + geom_boxplot(alpha=0.3)





# Step12 핵심인자 선정  --------------------------------------------------------
## 데이터 Set 구분하기

save(df, file = "df")
write.csv(df, "df.csv")

rm(list=ls())

load("df")


str(df)
dim(df)
nrow(df)
set.seed(7279)
train=sample(nrow(df),nrow(df)*0.7)
train

test=(1:c(nrow(df)))[-train]
test

length(train)
length(test)

df_train = df[train,]
df_test = df[test,]

head(df_train)
head(df_test)


# Step13 분석 모형 검토  -------------------------------------------------------

# 아래는 각 인자별 관계를 확인하지 않고 그냥 DO가 Project Y 나머지를 그냥 기존 데이터로 생각하고 돌린 부분입니다. 이와 비슷하게 정리하려고 하오니, 해석에 오해가없도록 참조만 하세요. 

# ■ Improve ----------------------------------------------------------------------
# Step14 최적 모형 수립  -------------------------------------------------------
## 분석실시(modeling) regression, rf
lm.fit=lm(y~.,data=df[train,-1])
step(lm.fit)
lm.fit_best = lm(y ~ f2 + tmp + oper + mlss + toc1 + toc2, data = df[train, -1])

# 하지만 TN, toc1, toc2는 조절 불가 따라서 변수에서 삭제, 공변량으로 넣는 것은 체크 
lm.fit_best = lm(y ~ f2 + tmp + oper + mlss, data = df[train, -1])
summary(lm.fit_best)

library(randomForest) ; library(tree)
rf.fit=randomForest(y~.,data=df[train,-1],importance=T)
rf.fit
# importance(rf.fit)
varImpPlot(rf.fit)

rf.fit_best=randomForest(y~ TN + f2 + tmp + oper + f1 + toc1,data=df[train,-1],importance=T)
varImpPlot(rf.fit_best)

# 하지만 TN, toc1, toc2는 조절 불가 따라서 변수에서 삭제, 공변량으로 넣는 것은 체크 
rf.fit_best=randomForest(y~ f2 + tmp + oper + f1,data=df[train,-1],importance=T)
varImpPlot(rf.fit_best)

# Step15 모형 검증 및 최적화  --------------------------------------------------

lm_obs = df[test,]$y #실제 관측값
lm_pred = predict(lm.fit_best,newdata=df[test,-1]) # 예측값 
rf_pred = predict(rf.fit_best,newdata=df[test,-1]) # 예측값 

library(DescTools)
MSE(lm_pred, lm_obs)
MSE(rf_pred, lm_obs)

RMSE(lm_pred, lm_obs) 
RMSE(rf_pred, lm_obs)

# (cor(lm_pred,lm_obs))^2  #상관계수 제곱
# (cor(rf_pred,lm_obs))^2  #상관계수 제곱

ncol(df)


colnames(df)
df_range = df %>% select(y, f1, f2, tmp, oper, mlss, toc1, toc2, TN)
dim(df_range)
head(df_range)

print("Feature range")
for( i in 1:10){
  A = df_range %>% filter(df_range[,i]>0) %>% .[,i] 
  B = A[A>boxplot(A)$stats[1]&A<boxplot(A)$stats[5]] %>% range()
  print(data.frame(names=colnames(df_range)[i] ,lower=B[1],upper=B[2]))
}

table(df$oper)  #oper 조건별 체크 



summary(lm.fit_best) # 부호 다시 체크 

new_lm=data.frame(f2 = 11.53425 ,      #음
                  tmp = 32.65625 ,   #양  
                  oper = 46,    #양
                  mlss = 7590) #음

               




new_lm


# 엔지니어 관점 추가 변경했을때 예측
library(tree)
head(df)
df_tr = tree(y~ f2 + tmp + oper + f1, data=df[train,-1])
plot(df_tr) ; text(df_tr)
df_tr2 = tree(y~ f2 + tmp + oper + f1, data=df[test,-1])
plot(df_tr2) ; text(df_tr2)

library(rpart)
df_tr_r = rpart(y~ f2 + tmp + oper + f1, data=df[train,-1])
plot(df_tr_r) ; text(df_tr_r)



# tmp는 부호 방향이 반대인데, 그래프 분석을 보면 높을수록 작아지지만, 가동률, 원수, 방출량 영향성이 섞여 있음 .  

# df_new %>% ggplot(aes(oper,y)) + geom_jitter(aes(col=tmp)) + geom_boxplot(alpha=0.3)


new_rf=data.frame(f2 = 11.53425 ,    #음  클수록 작아짐
                  tmp = 35.1375 ,  #음  클수록 작아짐   #lm과 부호 방향 반대 
                  oper = 31 ,      # 적정 조건 설정 필요
                  f1 = 426.375)    # 양 



#만약 oper만 고려하면  
df_tr_r = rpart(y~ oper, data=df[train,-1])
plot(df_tr_r) ; text(df_tr_r)


new_rf

print("Feature range")
for( i in 1:10){
  A = df_range %>% filter(df_range[,i]>0) %>% .[,i] 
  B = A[A>boxplot(A)$stats[1]&A<boxplot(A)$stats[5]] %>% range()
  print(data.frame(names=colnames(df_range)[i] ,lower=B[1],upper=B[2]))
}


#최적조건 구하기- 인자별 Range확인 (예측) 
predict(lm.fit_best,newdata=new_lm)
predict(rf.fit_best,newdata=new_rf)
# predict(rf.fit,newdata=new)




# Step16 개선 결과 검증(Pilot Test) --------------------------------------------


# ■ Control ----------------------------------------------------------------------
# Step17 최적모형 모니터링  ----------------------------------------------------

# Step18 표준화 및 수평전개  ---------------------------------------------------



# BB DMAIC --------------
# MBB 데이터분석과정 ---------------------------------------------------------
# 작성자 : 백인엽P,  최종 수정일 : 20. 6. 11
# Text encoding : UTF-8

getwd()
setwd("D:/#.Secure Work Folder/DX-LSS-Project/DMAIC데이터분석R과정/DAT")


# I. Define ------------------------------------------------------------------
# ● Pareto Chart ----------------------------------------------------------

library(qcc)
x2=c(33,52,7,5,43,4,3,1,2,6)
names(x2) =c("A","B","C","D","E","F","G","H","I","M")
pareto.chart(x2)             
            

# II. 기초통계 ----------------------------------------------------------------

# ● 기술통계 ------------------------------------------------------------------

B=read.table("1.2_Define_Statistics_descr.txt",,header=T)
B=as.vector(t(B))
summary(B)

list("사분위 수"=quantile(B),"평균"=mean(B), "표준편차"=sd(B))

rm(list=ls())

# ● 이산형 확률분포 ----------------------------------------------------------
library(dplyr)

# dbinom(x, size, prob) # 이산확률
dbinom(0:60,60,0.15) %>% plot()
dbinom(0,60,9/60)

# pbinom(x, size, prob) # 이산누적확률
pbinom(3, 60, 9/60)

# ppois(x, lambda) # 포아송 분포
ppois(0,2)

# 이항분포 비교
pbinom(0, 95, 2/95)

# 55cm 길어서 불량인 확률 평균 50, 표준편차 5, 허용한계 55
1-pnorm(55, 50, 5)


# Examples
require(graphics) 
sum(dbinom(46:54, 100, 0.5)) # Compute P(45 < X < 55) for X Binomial(100,0.5)

## Using "log = TRUE" for an extended range :
n <- 2000
k <- seq(0, n, by = 20)
plot (k, dbinom(k, n, pi/10, log = TRUE), type = "l", ylab = "log density",
      main = "dbinom(*, log=TRUE) is better than  log(dbinom(*))")
lines(k, log(dbinom(k, n, pi/10)), col = "red", lwd = 2)

## extreme points are omitted since dbinom gives 0.
mtext("dbinom(k, log=TRUE)", adj = 0)
mtext("extended range", adj = 0, line = -1, font = 4)
mtext("log(dbinom(k))", col = "red", adj = 1)

dev.off()
rm(list=ls())


# III. Measure ----------------------------------------------------------
# ● Gage R&R ----------------------------------------------------------

#Install.packages(SixSigma)
library(SixSigma)
Ex2.1=read.table("2.1_Measure_Gage R&R.txt",header=T)
ss.rr(var=Response,part=Part,appr=Appraiser,lsl=15.7,usl=17.3,data=Ex2.1)

# ● 공정능력분석 ----------------------------------------------------------

Ex2.3=read.table("2.3_Measure_ProcessCapability.txt",header=T) 
ss.study.ca(xST=Ex2.3$Response, LSL = 15.7, USL = 17.3, Target = 16.5)

rm(list=ls())
dev.off()


# IV. Analyze ----------------------------------------------------------

# ● 그래프분석 ----------------------------------------------------------

# 히스토 그램
Ex3.1=read.table("3.1_Analyze_Graph_hyphothesis.txt",header=T)
head(Ex3.1)
hist(Ex3.1$P.Sales,main="Histogram of P.Sales",xlab="P.Sales",label=T)

qqnorm(Ex3.1$P.Sales) ; qqline(Ex3.1$P.Sales) ; shapiro.test(Ex3.1$P.Sales)

EX_yes = Ex3.1 %>% filter(Training == "Yes")
EX_no = Ex3.1 %>% filter(Training == "No")

opar=par(mfrow=c(2,1))
hist(EX_yes$A.Sales,main="교육 전",xlab="P.Sales",label=T, xlim=c(10000,50000))
hist(EX_no$A.Sales,main="교육 후",xlab="P.Sales",label=T, xlim=c(10000,50000))

par(opar)
dev.off()


# 상자 그림
boxplot(Ex3.1$P.Sales, horizontal=T)
dev.off()

head(Ex3.1)


A=stack(Ex3.1[,2:3])
plot(values~ind, A, horizontal=T, col="gray", xlab="",main="boxplot of P-Sale, A-Sale")


# 산점도
plot(Ex3.1$A.Price,Ex3.1$A.Profit,xlab="A.Price",ylab="A.Profit")
plot(Ex3.1$A.Price,Ex3.1$A.Profit,xlab="A.Price",ylab="A.Profit",col=ifelse(Ex3.1$Training=="Yes",2,4),pch=19)
dev.off()

# 상관행렬도
pairs ( Ex3.1[,c(2,3,6,7)] )
pairs(Ex3.1[,c(2,3,6,7)],col=ifelse(Ex3.1$Training=="Yes",2,4),pch=19, main="Matrix plot of P.A Sales & A.Price, A Profit")
pairs(Ex3.1[,c(2,3,6,7)],col=ifelse(Ex3.1$Region=="West",2,4),pch=19, main="Matrix plot of P.A Sales & A.Price, A Profit")


# ● 상관관계 ----------------------------------------------------------
dir()
Ex3.2=read.table("3.2_Analyze_Normality correction.txt",header=T)
head(Ex3.2)

shapiro.test(Ex3.2$KO) 

par(mfrow=c(1,2))
qqnorm( Ex3.2$KO) ; qqline(Ex3.2$KO) ; hist(Ex3.2$KO)
qqnorm( Ex3.2$IN) ; qqline(Ex3.2$IN) ; hist(Ex3.2$IN)
qqnorm( Ex3.2$MA) ; qqline(Ex3.2$MA) ; hist(Ex3.2$MA)
qqnorm( Ex3.2$MC) ; qqline(Ex3.2$MC) ; hist(Ex3.2$MC)
dev.off()


# ● Box Cox 변환----------------------------------------------------------
library(forecast)
BoxCox(Ex3.2$IN,lambda="auto")
shapiro.test(BoxCox(Ex3.2$IN,lambda="auto"))

shapiro.test (BoxCox(Ex3.2$MA,lambda="auto"))
              
shapiro.test(Ex3.2$MC[Ex3.2$MC.C=="OA"])
shapiro.test(Ex3.2$MC[Ex3.2$MC.C=="IA"])
shapiro.test(Ex3.2$MC[Ex3.2$MC.C=="KW"])
shapiro.test(Ex3.2$MC[Ex3.2$MC.C=="SU"])

opar = par(mfrow=c(1,4))
qqnorm(Ex3.2$MC[Ex3.2$MC.C=="OA"]);qqline(Ex3.2$MC[Ex3.2$MC.C=="OA"])
qqnorm(Ex3.2$MC[Ex3.2$MC.C=="IA"]);qqline(Ex3.2$MC[Ex3.2$MC.C=="IA"])
qqnorm(Ex3.2$MC[Ex3.2$MC.C=="KW"]);qqline(Ex3.2$MC[Ex3.2$MC.C=="KW"])
qqnorm(Ex3.2$MC[Ex3.2$MC.C=="SU"]);qqline(Ex3.2$MC[Ex3.2$MC.C=="SU"])
dev.off()


# ● t 검정----------------------------------------------------------
# t-test
Ex3.3=read.table("3.3_Analyze_Hyphothesis.txt",header=T)
t.test(Ex3.3$T.Length,mu=36.75)


library(BSDA)
z.test(Ex3.3$T.Length,sigma.x=2.5,mu=36.75)


A=Ex3.3$T.Length[Ex3.3$Suplier=="A"] ;  B=Ex3.3$T.Length[Ex3.3$Suplier=="B"]
var.test (A, B )


t.test(A,B, var.equal=T)

#Paired-t
t.test (Ex3.3$T.Length, Ex3.3$D.Length, paired=T)


# ● ANOVA ----------------------------------------------------------

#install.packages("phia") 
library(phia)

Ex3.3.2 = read.table("3.3.2_Analyze_Hyphothesis_ANOVA.txt")
plot(interactionMeans(aov(Strength~Delay.T,data=Ex3.3.2)))

anova.fit=aov(Strength~Delay.T,data=Ex3.3.2)
summary(anova.fit)
opar = par(mfrow=c(2,2))
plot(anova.fit)

# 등분산성 검정
bartlett.test(Strength~Delay.T,data=Ex3.3.2)
leveneTest(Strength~Delay.T,data=Ex3.3.2)


# Anova
anova.fit=aov(Strength~F.Temperature, data=Ex3.3.2)
summary(anova.fit) ; plot(anova.fit)


anova.fit=aov(Strength~C.RPM, data=Ex3.3.2)
summary(anova.fit) ; plot(anova.fit)


# ● 비율검정 ----------------------------------------------------------

# 1-proportion test
Ex3.3=read.table("3.3_Analyze_Hyphothesis.txt", header=T)
table ( Ex3.3$PASS) 
prop.test(52, 58, p=0.75, alternative="greater")
prop.test(52, 58, p=0.85, alternative="greater")


# 2-proportion test
table ( Ex3.3$PASS, Ex3.3$Suplier)
x= c(24,28)
n=c(29,29) 
prop.test(x,n)


#Chi-square Test
A=tapply(Ex3.3$Spot.25,list(Ex3.3$C.RPM,Ex3.3$Delay.T),sum)
chisq.test ( A )


A=matrix(c(15,26,33,21,31,17,45,34,49,13,5,20),nrow=3)
colnames(A)=c("A","B","C","D")
rownames(A)=c(1,2,3)
chisq.test(A)


# ● 회귀분석 ----------------------------------------------------------

#install.packages("Hmisc")
library("Hmisc")
dir()
Ex3.4 = read.table("3.4_Analyze_Regression.txt", header = TRUE)
rcorr(as.matrix(Ex3.4[, 2:6]))
pairs ( Ex3.4 [ , 2:6])


head(Ex3.4)
# linear Regression
lm.fit=lm( Response~Ing3, data=Ex3.4)
summary(lm.fit)
plot(lm.fit)


lm.fit=lm( Response~Ing2, data=Ex3.4)
summary(lm.fit)
plot(lm.fit)


library(leaps)

reg.fit=regsubsets(Response~Ing1+Ing2+Ing3+Ing4+Ing5,data=Ex3.4)
summary(reg.fit)
plot(summary(reg.fit)$rsq) ;   plot(summary(reg.fit)$rss)
dev.off()


#조금 이쁜 그림 그리기 
par(mfrow=c(1,2))
plot(summary(reg.fit)$rsq,type="b",xlab="Number of variables", ylab="R-Square")
points(2,summary(reg.fit)$rsq[2],col="red",cex=2,pch=20)
plot(summary(reg.fit)$rss,type="b",xlab="Number of variables", ylab="SSE")
points(3,summary(reg.fit)$rss[3],col="red",cex=2,pch=20)
dev.off()


# plot type option
# "p" for points,
# "l" for lines,
# "b" for both,
# "c" for the lines part alone of "b",
# "o" for both ‘overplotted’,
# "h" for ‘histogram’ like (or ‘high-density’) vertical lines,
# "s" for stair steps,
# "S" for other steps, see ‘Details’ below,
# "n" for no plotting.



#V. Improve -----------------------------------------------------------------

# ● Regression ----------------------------------------------------------
#Regression
library(car)
reg.fit2=lm(Response~Ing2+Ing3+Ing5,data=Ex3.4)
vif(reg.fit2)
summary(reg.fit2)
opar=par(mfrow=c(2,2))
plot(reg.fit2) 

summary( lm(Response~Ing1+Ing2+Ing3+Ing4+Ing5,data=Ex3.4))
vif (lm(Response~Ing1+Ing2+Ing3+Ing4+Ing5,data=Ex3.4))


# ● 실험계획법----------------------------------------------------------
library(DoE.base)
doe_design=fac.design(factor.names=list(Time=c(60,90,120),Volt=c(115,220)),
                      replications=2,randomize=F)
doe_design

result=c(82,59,38,92,40,38,84,62,43,88,42,36)
test_result=add.response(doe_design,response=result)
test_result


#DOE
library(phia)
plot (interactionMeans (aov (result~Time*Volt, data=test_result ) ) )

DoE.result=aov(result~Time*Volt, data=test_result )
summary(DoE.result)
plot(DoE.result)

dev.off()

DoE.result=aov(result~Time*Volt + Error(Time), data=test_result )
summary(DoE.result)

DoE_2Level=fac.design(factor.names=list(A=c("T1","T2"),B=c("Sliver","Black"),
                                        C=c("B","M"),D=c("on","off")),randomize=F)
DoE_2Level

result2=c(8.39,8.4,9.58,9.13,8.15,7.95,8.87,9.36,7.88,7.38,8.32,8.87,7.96,8.26,9.2,8.81) 
test_result2=add.response(DoE_2Level, response=result2)
test_result2

DoE.result2=aov(result2~A*B*C*D, data=test_result2 )
summary(DoE.result2)


DoE.result3=update(DoE.result2,~.-A:B:C:D)
summary(DoE.result3)

DoE.result4=update(DoE.result3,~.-A:B:C) ; DoE.result4=update(DoE.result4,~.-A:B:D)
DoE.result4=update(DoE.result4,~.-A:C:D) ; DoE.result4=update(DoE.result4,~.-B:C:D)
summary(DoE.result4)


DoE.result5=update(DoE.result4,~.-B:C) ; DoE.result5=update(DoE.result5,~.-A:D)
DoE.result5=update(DoE.result5,~.-B:D) ; DoE.result5=update(DoE.result5,~.-A:B)
DoE.result5=update(DoE.result5,~.-A:C)
summary(DoE.result5)
plot (DoE.result5)

# ● RCBD ----------------------------------------------------------

library(phia)
plot (interactionMeans (DoE.result5))

RCBD=fac.design(factor.names=list(Supplier=c("S1","S2","S3","S4"),
                                  Temp=c("T1","T2","T3")),randomize=F)
RCBD
result=c(20,27,36,36,31,33,37,38,30,41,38,43)
test_result=add.response(RCBD,result)
test_result


DoE.result=aov(result~Supplier+Temp,data=test_result)
summary(DoE.result)

DoE.result=aov(result~Supplier+Temp,data=test_result)
plot(interactionMeans(DoE.result))
dev.off()
opar = par(mfrow=c(2,2))
plot (DoE.result)
par(opar)
dev.off()



# ● ANOCOVA ----------------------------------------------------------
dir()
Ex4.4 = read.table("4.4_Improve_Covariate.txt", header = TRUE)
attach(test_result) ; tapply(result,Temp,mean)

plot(Response~Thickness,data=Ex4.4,col=Formulation,pch=19)
plot(Response~Diameter,data=Ex4.4,col=Formulation,pch=19)


attach(Ex4.4)
plot(tapply(Response,Formulation,mean), type="b")
abline(h=47.5,col="red",lty="dotted")
detach(Ex4.4)

covari= lm(Response~Thickness+factor(Formulation),data=Ex4.4)
summary.aov(covari) 
opar = par(mfrow=c(2,2))
plot(covari)



#VI. Control -----------------------------------------------------------------

# ● 연속형 관리도  ----------------------------------------------------------
library(qcc)
Ex5.1=read.table("5.1_Contorl_Xbar_R.txt",header=T)
Ex5.1.1=with( Ex5.1, qcc.groups(Noise, Date))
qcc (Ex5.1.1, type="xbar" )
qcc(Ex5.1.1, type="R")
dev.off()



Ex5.2=read.table("5.2_Contorl_Xbar_R.txt",header=T)
Ex5.2.2=with( Ex5.2, qcc.groups(Noise, Date))
qcc(Ex5.2.2[1:14, ], type="xbar"); qcc(Ex5.2.2[15:28,],type="xbar")
qcc(Ex5.2.2[1:14, ], type="R"); qcc(Ex5.2.2[15:28,],type="R")
dev.off()


Ex5.2_IMR=read.table("5.2_IMR.txt" ,header=T)
qcc(Ex5.2_IMR,type="xbar.one")
Ex5.2_MR=cbind(Ex5.2_IMR[1:29,1],Ex5.2_IMR[2:30,1])
qcc(Ex5.2_MR,type="R")
dev.off()

# ● 이산형 관리도 ----------------------------------------------------------

Ex5.3=read.table("5.3_NP.txt",header=T)
with(Ex5.3,qcc(NC,sizes=50,type="np"))
dev.off()

Ex5.4=read.table("5.4_P.txt",header=T) #교제 오류 있음
with(Ex5.4,qcc(NC,sizes=Sample,type="p"))
dev.off()

Ex5.5=read.table("5.5_C.txt",header=T)
with(Ex5.5,qcc(NC ,type="c"))

Ex5.6=read.table("5.6_U.txt" ,header=T)
with(Ex5.6,qcc(NC,sizes=No.Unit,type="u"))




